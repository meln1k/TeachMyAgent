<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>TeachMyAgent.students.openai_baselines.common.distributions API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://github.com/flowersteam/TeachMyAgent/blob/gh-pages/images/favicon-96x96.png?raw=true" />
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TeachMyAgent.students.openai_baselines.common.distributions</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tensorflow as tf
import numpy as np
import TeachMyAgent.students.openai_baselines.common.tf_util as U
from TeachMyAgent.students.openai_baselines.a2c.utils import fc
from tensorflow.python.ops import math_ops

class Pd(object):
    &#34;&#34;&#34;
    A particular probability distribution
    &#34;&#34;&#34;
    def flatparam(self):
        raise NotImplementedError
    def mode(self):
        raise NotImplementedError
    def neglogp(self, x):
        # Usually it&#39;s easier to define the negative logprob
        raise NotImplementedError
    def kl(self, other):
        raise NotImplementedError
    def entropy(self):
        raise NotImplementedError
    def sample(self):
        raise NotImplementedError
    def logp(self, x):
        return - self.neglogp(x)
    def get_shape(self):
        return self.flatparam().shape
    @property
    def shape(self):
        return self.get_shape()
    def __getitem__(self, idx):
        return self.__class__(self.flatparam()[idx])

class PdType(object):
    &#34;&#34;&#34;
    Parametrized family of probability distributions
    &#34;&#34;&#34;
    def pdclass(self):
        raise NotImplementedError
    def pdfromflat(self, flat):
        return self.pdclass()(flat)
    def pdfromlatent(self, latent_vector, init_scale, init_bias):
        raise NotImplementedError
    def param_shape(self):
        raise NotImplementedError
    def sample_shape(self):
        raise NotImplementedError
    def sample_dtype(self):
        raise NotImplementedError

    def param_placeholder(self, prepend_shape, name=None):
        return tf.placeholder(dtype=tf.float32, shape=prepend_shape+self.param_shape(), name=name)
    def sample_placeholder(self, prepend_shape, name=None):
        return tf.placeholder(dtype=self.sample_dtype(), shape=prepend_shape+self.sample_shape(), name=name)

    def __eq__(self, other):
        return (type(self) == type(other)) and (self.__dict__ == other.__dict__)

class CategoricalPdType(PdType):
    def __init__(self, ncat):
        self.ncat = ncat
    def pdclass(self):
        return CategoricalPd
    def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
        pdparam = _matching_fc(latent_vector, &#39;pi&#39;, self.ncat, init_scale=init_scale, init_bias=init_bias)
        return self.pdfromflat(pdparam), pdparam

    def param_shape(self):
        return [self.ncat]
    def sample_shape(self):
        return []
    def sample_dtype(self):
        return tf.int32


class MultiCategoricalPdType(PdType):
    def __init__(self, nvec):
        self.ncats = nvec.astype(&#39;int32&#39;)
        assert (self.ncats &gt; 0).all()
    def pdclass(self):
        return MultiCategoricalPd
    def pdfromflat(self, flat):
        return MultiCategoricalPd(self.ncats, flat)

    def pdfromlatent(self, latent, init_scale=1.0, init_bias=0.0):
        pdparam = _matching_fc(latent, &#39;pi&#39;, self.ncats.sum(), init_scale=init_scale, init_bias=init_bias)
        return self.pdfromflat(pdparam), pdparam

    def param_shape(self):
        return [sum(self.ncats)]
    def sample_shape(self):
        return [len(self.ncats)]
    def sample_dtype(self):
        return tf.int32

class DiagGaussianPdType(PdType):
    def __init__(self, size):
        self.size = size
    def pdclass(self):
        return DiagGaussianPd

    def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
        mean = _matching_fc(latent_vector, &#39;pi&#39;, self.size, init_scale=init_scale, init_bias=init_bias)
        logstd = tf.get_variable(name=&#39;pi/logstd&#39;, shape=[1, self.size], initializer=tf.zeros_initializer())
        pdparam = tf.concat([mean, mean * 0.0 + logstd], axis=1)
        return self.pdfromflat(pdparam), mean

    def param_shape(self):
        return [2*self.size]
    def sample_shape(self):
        return [self.size]
    def sample_dtype(self):
        return tf.float32

class BernoulliPdType(PdType):
    def __init__(self, size):
        self.size = size
    def pdclass(self):
        return BernoulliPd
    def param_shape(self):
        return [self.size]
    def sample_shape(self):
        return [self.size]
    def sample_dtype(self):
        return tf.int32
    def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
        pdparam = _matching_fc(latent_vector, &#39;pi&#39;, self.size, init_scale=init_scale, init_bias=init_bias)
        return self.pdfromflat(pdparam), pdparam

# WRONG SECOND DERIVATIVES
# class CategoricalPd(Pd):
#     def __init__(self, logits):
#         self.logits = logits
#         self.ps = tf.nn.softmax(logits)
#     @classmethod
#     def fromflat(cls, flat):
#         return cls(flat)
#     def flatparam(self):
#         return self.logits
#     def mode(self):
#         return U.argmax(self.logits, axis=-1)
#     def logp(self, x):
#         return -tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits, x)
#     def kl(self, other):
#         return tf.nn.softmax_cross_entropy_with_logits(other.logits, self.ps) \
#                 - tf.nn.softmax_cross_entropy_with_logits(self.logits, self.ps)
#     def entropy(self):
#         return tf.nn.softmax_cross_entropy_with_logits(self.logits, self.ps)
#     def sample(self):
#         u = tf.random_uniform(tf.shape(self.logits))
#         return U.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)

class CategoricalPd(Pd):
    def __init__(self, logits):
        self.logits = logits
    def flatparam(self):
        return self.logits
    def mode(self):
        return tf.argmax(self.logits, axis=-1)

    @property
    def mean(self):
        return tf.nn.softmax(self.logits)
    def neglogp(self, x):
        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)
        # Note: we can&#39;t use sparse_softmax_cross_entropy_with_logits because
        #       the implementation does not allow second-order derivatives...
        if x.dtype in {tf.uint8, tf.int32, tf.int64}:
            # one-hot encoding
            x_shape_list = x.shape.as_list()
            logits_shape_list = self.logits.get_shape().as_list()[:-1]
            for xs, ls in zip(x_shape_list, logits_shape_list):
                if xs is not None and ls is not None:
                    assert xs == ls, &#39;shape mismatch: {} in x vs {} in logits&#39;.format(xs, ls)

            x = tf.one_hot(x, self.logits.get_shape().as_list()[-1])
        else:
            # already encoded
            assert x.shape.as_list() == self.logits.shape.as_list()

        return tf.nn.softmax_cross_entropy_with_logits_v2(
            logits=self.logits,
            labels=x)
    def kl(self, other):
        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)
        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keepdims=True)
        ea0 = tf.exp(a0)
        ea1 = tf.exp(a1)
        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)
        z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)
        p0 = ea0 / z0
        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)
    def entropy(self):
        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)
        ea0 = tf.exp(a0)
        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)
        p0 = ea0 / z0
        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)
    def sample(self):
        u = tf.random_uniform(tf.shape(self.logits), dtype=self.logits.dtype)
        return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)
    @classmethod
    def fromflat(cls, flat):
        return cls(flat)

class MultiCategoricalPd(Pd):
    def __init__(self, nvec, flat):
        self.flat = flat
        self.categoricals = list(map(CategoricalPd,
            tf.split(flat, np.array(nvec, dtype=np.int32), axis=-1)))
    def flatparam(self):
        return self.flat
    def mode(self):
        return tf.cast(tf.stack([p.mode() for p in self.categoricals], axis=-1), tf.int32)
    def neglogp(self, x):
        return tf.add_n([p.neglogp(px) for p, px in zip(self.categoricals, tf.unstack(x, axis=-1))])
    def kl(self, other):
        return tf.add_n([p.kl(q) for p, q in zip(self.categoricals, other.categoricals)])
    def entropy(self):
        return tf.add_n([p.entropy() for p in self.categoricals])
    def sample(self):
        return tf.cast(tf.stack([p.sample() for p in self.categoricals], axis=-1), tf.int32)
    @classmethod
    def fromflat(cls, flat):
        raise NotImplementedError

class DiagGaussianPd(Pd):
    def __init__(self, flat):
        self.flat = flat
        mean, logstd = tf.split(axis=len(flat.shape)-1, num_or_size_splits=2, value=flat)
        self.mean = mean
        self.logstd = logstd
        self.std = tf.exp(logstd)
    def flatparam(self):
        return self.flat
    def mode(self):
        return self.mean
    def neglogp(self, x):
        return 0.5 * tf.reduce_sum(tf.square((x - self.mean) / self.std), axis=-1) \
               + 0.5 * np.log(2.0 * np.pi) * tf.to_float(tf.shape(x)[-1]) \
               + tf.reduce_sum(self.logstd, axis=-1)
    def kl(self, other):
        assert isinstance(other, DiagGaussianPd)
        return tf.reduce_sum(other.logstd - self.logstd + (tf.square(self.std) + tf.square(self.mean - other.mean)) / (2.0 * tf.square(other.std)) - 0.5, axis=-1)
    def entropy(self):
        return tf.reduce_sum(self.logstd + .5 * np.log(2.0 * np.pi * np.e), axis=-1)
    def sample(self):
        return self.mean + self.std * tf.random_normal(tf.shape(self.mean))
    @classmethod
    def fromflat(cls, flat):
        return cls(flat)


class BernoulliPd(Pd):
    def __init__(self, logits):
        self.logits = logits
        self.ps = tf.sigmoid(logits)
    def flatparam(self):
        return self.logits
    @property
    def mean(self):
        return self.ps
    def mode(self):
        return tf.round(self.ps)
    def neglogp(self, x):
        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.to_float(x)), axis=-1)
    def kl(self, other):
        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=other.logits, labels=self.ps), axis=-1) - tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.ps), axis=-1)
    def entropy(self):
        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.ps), axis=-1)
    def sample(self):
        u = tf.random_uniform(tf.shape(self.ps))
        return tf.to_float(math_ops.less(u, self.ps))
    @classmethod
    def fromflat(cls, flat):
        return cls(flat)

def make_pdtype(ac_space):
    from gym import spaces
    if isinstance(ac_space, spaces.Box):
        assert len(ac_space.shape) == 1
        return DiagGaussianPdType(ac_space.shape[0])
    elif isinstance(ac_space, spaces.Discrete):
        return CategoricalPdType(ac_space.n)
    elif isinstance(ac_space, spaces.MultiDiscrete):
        return MultiCategoricalPdType(ac_space.nvec)
    elif isinstance(ac_space, spaces.MultiBinary):
        return BernoulliPdType(ac_space.n)
    else:
        raise NotImplementedError

def shape_el(v, i):
    maybe = v.get_shape()[i]
    if maybe is not None:
        return maybe
    else:
        return tf.shape(v)[i]

@U.in_session
def test_probtypes():
    np.random.seed(0)

    pdparam_diag_gauss = np.array([-.2, .3, .4, -.5, .1, -.5, .1, 0.8])
    diag_gauss = DiagGaussianPdType(pdparam_diag_gauss.size // 2) #pylint: disable=E1101
    validate_probtype(diag_gauss, pdparam_diag_gauss)

    pdparam_categorical = np.array([-.2, .3, .5])
    categorical = CategoricalPdType(pdparam_categorical.size) #pylint: disable=E1101
    validate_probtype(categorical, pdparam_categorical)

    nvec = [1,2,3]
    pdparam_multicategorical = np.array([-.2, .3, .5, .1, 1, -.1])
    multicategorical = MultiCategoricalPdType(nvec) #pylint: disable=E1101
    validate_probtype(multicategorical, pdparam_multicategorical)

    pdparam_bernoulli = np.array([-.2, .3, .5])
    bernoulli = BernoulliPdType(pdparam_bernoulli.size) #pylint: disable=E1101
    validate_probtype(bernoulli, pdparam_bernoulli)


def validate_probtype(probtype, pdparam):
    N = 100000
    # Check to see if mean negative log likelihood == differential entropy
    Mval = np.repeat(pdparam[None, :], N, axis=0)
    M = probtype.param_placeholder([N])
    X = probtype.sample_placeholder([N])
    pd = probtype.pdfromflat(M)
    calcloglik = U.function([X, M], pd.logp(X))
    calcent = U.function([M], pd.entropy())
    Xval = tf.get_default_session().run(pd.sample(), feed_dict={M:Mval})
    logliks = calcloglik(Xval, Mval)
    entval_ll = - logliks.mean() #pylint: disable=E1101
    entval_ll_stderr = logliks.std() / np.sqrt(N) #pylint: disable=E1101
    entval = calcent(Mval).mean() #pylint: disable=E1101
    assert np.abs(entval - entval_ll) &lt; 3 * entval_ll_stderr # within 3 sigmas

    # Check to see if kldiv[p,q] = - ent[p] - E_p[log q]
    M2 = probtype.param_placeholder([N])
    pd2 = probtype.pdfromflat(M2)
    q = pdparam + np.random.randn(pdparam.size) * 0.1
    Mval2 = np.repeat(q[None, :], N, axis=0)
    calckl = U.function([M, M2], pd.kl(pd2))
    klval = calckl(Mval, Mval2).mean() #pylint: disable=E1101
    logliks = calcloglik(Xval, Mval2)
    klval_ll = - entval - logliks.mean() #pylint: disable=E1101
    klval_ll_stderr = logliks.std() / np.sqrt(N) #pylint: disable=E1101
    assert np.abs(klval - klval_ll) &lt; 3 * klval_ll_stderr # within 3 sigmas
    print(&#39;ok on&#39;, probtype, pdparam)


def _matching_fc(tensor, name, size, init_scale, init_bias):
    if tensor.shape[-1] == size:
        return tensor
    else:
        return fc(tensor, name, size, init_scale=init_scale, init_bias=init_bias)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.make_pdtype"><code class="name flex">
<span>def <span class="ident">make_pdtype</span></span>(<span>ac_space)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_pdtype(ac_space):
    from gym import spaces
    if isinstance(ac_space, spaces.Box):
        assert len(ac_space.shape) == 1
        return DiagGaussianPdType(ac_space.shape[0])
    elif isinstance(ac_space, spaces.Discrete):
        return CategoricalPdType(ac_space.n)
    elif isinstance(ac_space, spaces.MultiDiscrete):
        return MultiCategoricalPdType(ac_space.nvec)
    elif isinstance(ac_space, spaces.MultiBinary):
        return BernoulliPdType(ac_space.n)
    else:
        raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.shape_el"><code class="name flex">
<span>def <span class="ident">shape_el</span></span>(<span>v, i)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shape_el(v, i):
    maybe = v.get_shape()[i]
    if maybe is not None:
        return maybe
    else:
        return tf.shape(v)[i]</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.test_probtypes"><code class="name flex">
<span>def <span class="ident">test_probtypes</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@U.in_session
def test_probtypes():
    np.random.seed(0)

    pdparam_diag_gauss = np.array([-.2, .3, .4, -.5, .1, -.5, .1, 0.8])
    diag_gauss = DiagGaussianPdType(pdparam_diag_gauss.size // 2) #pylint: disable=E1101
    validate_probtype(diag_gauss, pdparam_diag_gauss)

    pdparam_categorical = np.array([-.2, .3, .5])
    categorical = CategoricalPdType(pdparam_categorical.size) #pylint: disable=E1101
    validate_probtype(categorical, pdparam_categorical)

    nvec = [1,2,3]
    pdparam_multicategorical = np.array([-.2, .3, .5, .1, 1, -.1])
    multicategorical = MultiCategoricalPdType(nvec) #pylint: disable=E1101
    validate_probtype(multicategorical, pdparam_multicategorical)

    pdparam_bernoulli = np.array([-.2, .3, .5])
    bernoulli = BernoulliPdType(pdparam_bernoulli.size) #pylint: disable=E1101
    validate_probtype(bernoulli, pdparam_bernoulli)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.validate_probtype"><code class="name flex">
<span>def <span class="ident">validate_probtype</span></span>(<span>probtype, pdparam)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_probtype(probtype, pdparam):
    N = 100000
    # Check to see if mean negative log likelihood == differential entropy
    Mval = np.repeat(pdparam[None, :], N, axis=0)
    M = probtype.param_placeholder([N])
    X = probtype.sample_placeholder([N])
    pd = probtype.pdfromflat(M)
    calcloglik = U.function([X, M], pd.logp(X))
    calcent = U.function([M], pd.entropy())
    Xval = tf.get_default_session().run(pd.sample(), feed_dict={M:Mval})
    logliks = calcloglik(Xval, Mval)
    entval_ll = - logliks.mean() #pylint: disable=E1101
    entval_ll_stderr = logliks.std() / np.sqrt(N) #pylint: disable=E1101
    entval = calcent(Mval).mean() #pylint: disable=E1101
    assert np.abs(entval - entval_ll) &lt; 3 * entval_ll_stderr # within 3 sigmas

    # Check to see if kldiv[p,q] = - ent[p] - E_p[log q]
    M2 = probtype.param_placeholder([N])
    pd2 = probtype.pdfromflat(M2)
    q = pdparam + np.random.randn(pdparam.size) * 0.1
    Mval2 = np.repeat(q[None, :], N, axis=0)
    calckl = U.function([M, M2], pd.kl(pd2))
    klval = calckl(Mval, Mval2).mean() #pylint: disable=E1101
    logliks = calcloglik(Xval, Mval2)
    klval_ll = - entval - logliks.mean() #pylint: disable=E1101
    klval_ll_stderr = logliks.std() / np.sqrt(N) #pylint: disable=E1101
    assert np.abs(klval - klval_ll) &lt; 3 * klval_ll_stderr # within 3 sigmas
    print(&#39;ok on&#39;, probtype, pdparam)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd"><code class="flex name class">
<span>class <span class="ident">BernoulliPd</span></span>
<span>(</span><span>logits)</span>
</code></dt>
<dd>
<div class="desc"><p>A particular probability distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BernoulliPd(Pd):
    def __init__(self, logits):
        self.logits = logits
        self.ps = tf.sigmoid(logits)
    def flatparam(self):
        return self.logits
    @property
    def mean(self):
        return self.ps
    def mode(self):
        return tf.round(self.ps)
    def neglogp(self, x):
        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.to_float(x)), axis=-1)
    def kl(self, other):
        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=other.logits, labels=self.ps), axis=-1) - tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.ps), axis=-1)
    def entropy(self):
        return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.ps), axis=-1)
    def sample(self):
        u = tf.random_uniform(tf.shape(self.ps))
        return tf.to_float(math_ops.less(u, self.ps))
    @classmethod
    def fromflat(cls, flat):
        return cls(flat)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd">Pd</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.fromflat"><code class="name flex">
<span>def <span class="ident">fromflat</span></span>(<span>flat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def fromflat(cls, flat):
    return cls(flat)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.mean"><code class="name">var <span class="ident">mean</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mean(self):
    return self.ps</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.entropy"><code class="name flex">
<span>def <span class="ident">entropy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entropy(self):
    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.ps), axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.flatparam"><code class="name flex">
<span>def <span class="ident">flatparam</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatparam(self):
    return self.logits</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.kl"><code class="name flex">
<span>def <span class="ident">kl</span></span>(<span>self, other)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kl(self, other):
    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=other.logits, labels=self.ps), axis=-1) - tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=self.ps), axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.mode"><code class="name flex">
<span>def <span class="ident">mode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mode(self):
    return tf.round(self.ps)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.neglogp"><code class="name flex">
<span>def <span class="ident">neglogp</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def neglogp(self, x):
    return tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.to_float(x)), axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self):
    u = tf.random_uniform(tf.shape(self.ps))
    return tf.to_float(math_ops.less(u, self.ps))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType"><code class="flex name class">
<span>class <span class="ident">BernoulliPdType</span></span>
<span>(</span><span>size)</span>
</code></dt>
<dd>
<div class="desc"><p>Parametrized family of probability distributions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BernoulliPdType(PdType):
    def __init__(self, size):
        self.size = size
    def pdclass(self):
        return BernoulliPd
    def param_shape(self):
        return [self.size]
    def sample_shape(self):
        return [self.size]
    def sample_dtype(self):
        return tf.int32
    def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
        pdparam = _matching_fc(latent_vector, &#39;pi&#39;, self.size, init_scale=init_scale, init_bias=init_bias)
        return self.pdfromflat(pdparam), pdparam</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType">PdType</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.param_shape"><code class="name flex">
<span>def <span class="ident">param_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def param_shape(self):
    return [self.size]</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.pdclass"><code class="name flex">
<span>def <span class="ident">pdclass</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdclass(self):
    return BernoulliPd</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.pdfromlatent"><code class="name flex">
<span>def <span class="ident">pdfromlatent</span></span>(<span>self, latent_vector, init_scale=1.0, init_bias=0.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
    pdparam = _matching_fc(latent_vector, &#39;pi&#39;, self.size, init_scale=init_scale, init_bias=init_bias)
    return self.pdfromflat(pdparam), pdparam</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.sample_dtype"><code class="name flex">
<span>def <span class="ident">sample_dtype</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_dtype(self):
    return tf.int32</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.sample_shape"><code class="name flex">
<span>def <span class="ident">sample_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_shape(self):
    return [self.size]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd"><code class="flex name class">
<span>class <span class="ident">CategoricalPd</span></span>
<span>(</span><span>logits)</span>
</code></dt>
<dd>
<div class="desc"><p>A particular probability distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CategoricalPd(Pd):
    def __init__(self, logits):
        self.logits = logits
    def flatparam(self):
        return self.logits
    def mode(self):
        return tf.argmax(self.logits, axis=-1)

    @property
    def mean(self):
        return tf.nn.softmax(self.logits)
    def neglogp(self, x):
        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)
        # Note: we can&#39;t use sparse_softmax_cross_entropy_with_logits because
        #       the implementation does not allow second-order derivatives...
        if x.dtype in {tf.uint8, tf.int32, tf.int64}:
            # one-hot encoding
            x_shape_list = x.shape.as_list()
            logits_shape_list = self.logits.get_shape().as_list()[:-1]
            for xs, ls in zip(x_shape_list, logits_shape_list):
                if xs is not None and ls is not None:
                    assert xs == ls, &#39;shape mismatch: {} in x vs {} in logits&#39;.format(xs, ls)

            x = tf.one_hot(x, self.logits.get_shape().as_list()[-1])
        else:
            # already encoded
            assert x.shape.as_list() == self.logits.shape.as_list()

        return tf.nn.softmax_cross_entropy_with_logits_v2(
            logits=self.logits,
            labels=x)
    def kl(self, other):
        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)
        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keepdims=True)
        ea0 = tf.exp(a0)
        ea1 = tf.exp(a1)
        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)
        z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)
        p0 = ea0 / z0
        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)
    def entropy(self):
        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)
        ea0 = tf.exp(a0)
        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)
        p0 = ea0 / z0
        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)
    def sample(self):
        u = tf.random_uniform(tf.shape(self.logits), dtype=self.logits.dtype)
        return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)
    @classmethod
    def fromflat(cls, flat):
        return cls(flat)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd">Pd</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.fromflat"><code class="name flex">
<span>def <span class="ident">fromflat</span></span>(<span>flat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def fromflat(cls, flat):
    return cls(flat)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.mean"><code class="name">var <span class="ident">mean</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def mean(self):
    return tf.nn.softmax(self.logits)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.entropy"><code class="name flex">
<span>def <span class="ident">entropy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entropy(self):
    a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)
    ea0 = tf.exp(a0)
    z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)
    p0 = ea0 / z0
    return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.flatparam"><code class="name flex">
<span>def <span class="ident">flatparam</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatparam(self):
    return self.logits</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.kl"><code class="name flex">
<span>def <span class="ident">kl</span></span>(<span>self, other)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kl(self, other):
    a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)
    a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keepdims=True)
    ea0 = tf.exp(a0)
    ea1 = tf.exp(a1)
    z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)
    z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)
    p0 = ea0 / z0
    return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.mode"><code class="name flex">
<span>def <span class="ident">mode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mode(self):
    return tf.argmax(self.logits, axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.neglogp"><code class="name flex">
<span>def <span class="ident">neglogp</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def neglogp(self, x):
    # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)
    # Note: we can&#39;t use sparse_softmax_cross_entropy_with_logits because
    #       the implementation does not allow second-order derivatives...
    if x.dtype in {tf.uint8, tf.int32, tf.int64}:
        # one-hot encoding
        x_shape_list = x.shape.as_list()
        logits_shape_list = self.logits.get_shape().as_list()[:-1]
        for xs, ls in zip(x_shape_list, logits_shape_list):
            if xs is not None and ls is not None:
                assert xs == ls, &#39;shape mismatch: {} in x vs {} in logits&#39;.format(xs, ls)

        x = tf.one_hot(x, self.logits.get_shape().as_list()[-1])
    else:
        # already encoded
        assert x.shape.as_list() == self.logits.shape.as_list()

    return tf.nn.softmax_cross_entropy_with_logits_v2(
        logits=self.logits,
        labels=x)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self):
    u = tf.random_uniform(tf.shape(self.logits), dtype=self.logits.dtype)
    return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType"><code class="flex name class">
<span>class <span class="ident">CategoricalPdType</span></span>
<span>(</span><span>ncat)</span>
</code></dt>
<dd>
<div class="desc"><p>Parametrized family of probability distributions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CategoricalPdType(PdType):
    def __init__(self, ncat):
        self.ncat = ncat
    def pdclass(self):
        return CategoricalPd
    def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
        pdparam = _matching_fc(latent_vector, &#39;pi&#39;, self.ncat, init_scale=init_scale, init_bias=init_bias)
        return self.pdfromflat(pdparam), pdparam

    def param_shape(self):
        return [self.ncat]
    def sample_shape(self):
        return []
    def sample_dtype(self):
        return tf.int32</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType">PdType</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.param_shape"><code class="name flex">
<span>def <span class="ident">param_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def param_shape(self):
    return [self.ncat]</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.pdclass"><code class="name flex">
<span>def <span class="ident">pdclass</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdclass(self):
    return CategoricalPd</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.pdfromlatent"><code class="name flex">
<span>def <span class="ident">pdfromlatent</span></span>(<span>self, latent_vector, init_scale=1.0, init_bias=0.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
    pdparam = _matching_fc(latent_vector, &#39;pi&#39;, self.ncat, init_scale=init_scale, init_bias=init_bias)
    return self.pdfromflat(pdparam), pdparam</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.sample_dtype"><code class="name flex">
<span>def <span class="ident">sample_dtype</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_dtype(self):
    return tf.int32</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.sample_shape"><code class="name flex">
<span>def <span class="ident">sample_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_shape(self):
    return []</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd"><code class="flex name class">
<span>class <span class="ident">DiagGaussianPd</span></span>
<span>(</span><span>flat)</span>
</code></dt>
<dd>
<div class="desc"><p>A particular probability distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DiagGaussianPd(Pd):
    def __init__(self, flat):
        self.flat = flat
        mean, logstd = tf.split(axis=len(flat.shape)-1, num_or_size_splits=2, value=flat)
        self.mean = mean
        self.logstd = logstd
        self.std = tf.exp(logstd)
    def flatparam(self):
        return self.flat
    def mode(self):
        return self.mean
    def neglogp(self, x):
        return 0.5 * tf.reduce_sum(tf.square((x - self.mean) / self.std), axis=-1) \
               + 0.5 * np.log(2.0 * np.pi) * tf.to_float(tf.shape(x)[-1]) \
               + tf.reduce_sum(self.logstd, axis=-1)
    def kl(self, other):
        assert isinstance(other, DiagGaussianPd)
        return tf.reduce_sum(other.logstd - self.logstd + (tf.square(self.std) + tf.square(self.mean - other.mean)) / (2.0 * tf.square(other.std)) - 0.5, axis=-1)
    def entropy(self):
        return tf.reduce_sum(self.logstd + .5 * np.log(2.0 * np.pi * np.e), axis=-1)
    def sample(self):
        return self.mean + self.std * tf.random_normal(tf.shape(self.mean))
    @classmethod
    def fromflat(cls, flat):
        return cls(flat)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd">Pd</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.fromflat"><code class="name flex">
<span>def <span class="ident">fromflat</span></span>(<span>flat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def fromflat(cls, flat):
    return cls(flat)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.entropy"><code class="name flex">
<span>def <span class="ident">entropy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entropy(self):
    return tf.reduce_sum(self.logstd + .5 * np.log(2.0 * np.pi * np.e), axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.flatparam"><code class="name flex">
<span>def <span class="ident">flatparam</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatparam(self):
    return self.flat</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.kl"><code class="name flex">
<span>def <span class="ident">kl</span></span>(<span>self, other)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kl(self, other):
    assert isinstance(other, DiagGaussianPd)
    return tf.reduce_sum(other.logstd - self.logstd + (tf.square(self.std) + tf.square(self.mean - other.mean)) / (2.0 * tf.square(other.std)) - 0.5, axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.mode"><code class="name flex">
<span>def <span class="ident">mode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mode(self):
    return self.mean</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.neglogp"><code class="name flex">
<span>def <span class="ident">neglogp</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def neglogp(self, x):
    return 0.5 * tf.reduce_sum(tf.square((x - self.mean) / self.std), axis=-1) \
           + 0.5 * np.log(2.0 * np.pi) * tf.to_float(tf.shape(x)[-1]) \
           + tf.reduce_sum(self.logstd, axis=-1)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self):
    return self.mean + self.std * tf.random_normal(tf.shape(self.mean))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType"><code class="flex name class">
<span>class <span class="ident">DiagGaussianPdType</span></span>
<span>(</span><span>size)</span>
</code></dt>
<dd>
<div class="desc"><p>Parametrized family of probability distributions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DiagGaussianPdType(PdType):
    def __init__(self, size):
        self.size = size
    def pdclass(self):
        return DiagGaussianPd

    def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
        mean = _matching_fc(latent_vector, &#39;pi&#39;, self.size, init_scale=init_scale, init_bias=init_bias)
        logstd = tf.get_variable(name=&#39;pi/logstd&#39;, shape=[1, self.size], initializer=tf.zeros_initializer())
        pdparam = tf.concat([mean, mean * 0.0 + logstd], axis=1)
        return self.pdfromflat(pdparam), mean

    def param_shape(self):
        return [2*self.size]
    def sample_shape(self):
        return [self.size]
    def sample_dtype(self):
        return tf.float32</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType">PdType</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.param_shape"><code class="name flex">
<span>def <span class="ident">param_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def param_shape(self):
    return [2*self.size]</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.pdclass"><code class="name flex">
<span>def <span class="ident">pdclass</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdclass(self):
    return DiagGaussianPd</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.pdfromlatent"><code class="name flex">
<span>def <span class="ident">pdfromlatent</span></span>(<span>self, latent_vector, init_scale=1.0, init_bias=0.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):
    mean = _matching_fc(latent_vector, &#39;pi&#39;, self.size, init_scale=init_scale, init_bias=init_bias)
    logstd = tf.get_variable(name=&#39;pi/logstd&#39;, shape=[1, self.size], initializer=tf.zeros_initializer())
    pdparam = tf.concat([mean, mean * 0.0 + logstd], axis=1)
    return self.pdfromflat(pdparam), mean</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.sample_dtype"><code class="name flex">
<span>def <span class="ident">sample_dtype</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_dtype(self):
    return tf.float32</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.sample_shape"><code class="name flex">
<span>def <span class="ident">sample_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_shape(self):
    return [self.size]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd"><code class="flex name class">
<span>class <span class="ident">MultiCategoricalPd</span></span>
<span>(</span><span>nvec, flat)</span>
</code></dt>
<dd>
<div class="desc"><p>A particular probability distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiCategoricalPd(Pd):
    def __init__(self, nvec, flat):
        self.flat = flat
        self.categoricals = list(map(CategoricalPd,
            tf.split(flat, np.array(nvec, dtype=np.int32), axis=-1)))
    def flatparam(self):
        return self.flat
    def mode(self):
        return tf.cast(tf.stack([p.mode() for p in self.categoricals], axis=-1), tf.int32)
    def neglogp(self, x):
        return tf.add_n([p.neglogp(px) for p, px in zip(self.categoricals, tf.unstack(x, axis=-1))])
    def kl(self, other):
        return tf.add_n([p.kl(q) for p, q in zip(self.categoricals, other.categoricals)])
    def entropy(self):
        return tf.add_n([p.entropy() for p in self.categoricals])
    def sample(self):
        return tf.cast(tf.stack([p.sample() for p in self.categoricals], axis=-1), tf.int32)
    @classmethod
    def fromflat(cls, flat):
        raise NotImplementedError</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd">Pd</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.fromflat"><code class="name flex">
<span>def <span class="ident">fromflat</span></span>(<span>flat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def fromflat(cls, flat):
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.entropy"><code class="name flex">
<span>def <span class="ident">entropy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entropy(self):
    return tf.add_n([p.entropy() for p in self.categoricals])</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.flatparam"><code class="name flex">
<span>def <span class="ident">flatparam</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatparam(self):
    return self.flat</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.kl"><code class="name flex">
<span>def <span class="ident">kl</span></span>(<span>self, other)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kl(self, other):
    return tf.add_n([p.kl(q) for p, q in zip(self.categoricals, other.categoricals)])</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.mode"><code class="name flex">
<span>def <span class="ident">mode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mode(self):
    return tf.cast(tf.stack([p.mode() for p in self.categoricals], axis=-1), tf.int32)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.neglogp"><code class="name flex">
<span>def <span class="ident">neglogp</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def neglogp(self, x):
    return tf.add_n([p.neglogp(px) for p, px in zip(self.categoricals, tf.unstack(x, axis=-1))])</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self):
    return tf.cast(tf.stack([p.sample() for p in self.categoricals], axis=-1), tf.int32)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType"><code class="flex name class">
<span>class <span class="ident">MultiCategoricalPdType</span></span>
<span>(</span><span>nvec)</span>
</code></dt>
<dd>
<div class="desc"><p>Parametrized family of probability distributions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiCategoricalPdType(PdType):
    def __init__(self, nvec):
        self.ncats = nvec.astype(&#39;int32&#39;)
        assert (self.ncats &gt; 0).all()
    def pdclass(self):
        return MultiCategoricalPd
    def pdfromflat(self, flat):
        return MultiCategoricalPd(self.ncats, flat)

    def pdfromlatent(self, latent, init_scale=1.0, init_bias=0.0):
        pdparam = _matching_fc(latent, &#39;pi&#39;, self.ncats.sum(), init_scale=init_scale, init_bias=init_bias)
        return self.pdfromflat(pdparam), pdparam

    def param_shape(self):
        return [sum(self.ncats)]
    def sample_shape(self):
        return [len(self.ncats)]
    def sample_dtype(self):
        return tf.int32</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType">PdType</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.param_shape"><code class="name flex">
<span>def <span class="ident">param_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def param_shape(self):
    return [sum(self.ncats)]</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdclass"><code class="name flex">
<span>def <span class="ident">pdclass</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdclass(self):
    return MultiCategoricalPd</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdfromflat"><code class="name flex">
<span>def <span class="ident">pdfromflat</span></span>(<span>self, flat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdfromflat(self, flat):
    return MultiCategoricalPd(self.ncats, flat)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdfromlatent"><code class="name flex">
<span>def <span class="ident">pdfromlatent</span></span>(<span>self, latent, init_scale=1.0, init_bias=0.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdfromlatent(self, latent, init_scale=1.0, init_bias=0.0):
    pdparam = _matching_fc(latent, &#39;pi&#39;, self.ncats.sum(), init_scale=init_scale, init_bias=init_bias)
    return self.pdfromflat(pdparam), pdparam</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.sample_dtype"><code class="name flex">
<span>def <span class="ident">sample_dtype</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_dtype(self):
    return tf.int32</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.sample_shape"><code class="name flex">
<span>def <span class="ident">sample_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_shape(self):
    return [len(self.ncats)]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd"><code class="flex name class">
<span>class <span class="ident">Pd</span></span>
</code></dt>
<dd>
<div class="desc"><p>A particular probability distribution</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Pd(object):
    &#34;&#34;&#34;
    A particular probability distribution
    &#34;&#34;&#34;
    def flatparam(self):
        raise NotImplementedError
    def mode(self):
        raise NotImplementedError
    def neglogp(self, x):
        # Usually it&#39;s easier to define the negative logprob
        raise NotImplementedError
    def kl(self, other):
        raise NotImplementedError
    def entropy(self):
        raise NotImplementedError
    def sample(self):
        raise NotImplementedError
    def logp(self, x):
        return - self.neglogp(x)
    def get_shape(self):
        return self.flatparam().shape
    @property
    def shape(self):
        return self.get_shape()
    def __getitem__(self, idx):
        return self.__class__(self.flatparam()[idx])</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd">BernoulliPd</a></li>
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd">CategoricalPd</a></li>
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd">DiagGaussianPd</a></li>
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd">MultiCategoricalPd</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.shape"><code class="name">var <span class="ident">shape</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self):
    return self.get_shape()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.entropy"><code class="name flex">
<span>def <span class="ident">entropy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entropy(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.flatparam"><code class="name flex">
<span>def <span class="ident">flatparam</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def flatparam(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.get_shape"><code class="name flex">
<span>def <span class="ident">get_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_shape(self):
    return self.flatparam().shape</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.kl"><code class="name flex">
<span>def <span class="ident">kl</span></span>(<span>self, other)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kl(self, other):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.logp"><code class="name flex">
<span>def <span class="ident">logp</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def logp(self, x):
    return - self.neglogp(x)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.mode"><code class="name flex">
<span>def <span class="ident">mode</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mode(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.neglogp"><code class="name flex">
<span>def <span class="ident">neglogp</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def neglogp(self, x):
    # Usually it&#39;s easier to define the negative logprob
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.Pd.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType"><code class="flex name class">
<span>class <span class="ident">PdType</span></span>
</code></dt>
<dd>
<div class="desc"><p>Parametrized family of probability distributions</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PdType(object):
    &#34;&#34;&#34;
    Parametrized family of probability distributions
    &#34;&#34;&#34;
    def pdclass(self):
        raise NotImplementedError
    def pdfromflat(self, flat):
        return self.pdclass()(flat)
    def pdfromlatent(self, latent_vector, init_scale, init_bias):
        raise NotImplementedError
    def param_shape(self):
        raise NotImplementedError
    def sample_shape(self):
        raise NotImplementedError
    def sample_dtype(self):
        raise NotImplementedError

    def param_placeholder(self, prepend_shape, name=None):
        return tf.placeholder(dtype=tf.float32, shape=prepend_shape+self.param_shape(), name=name)
    def sample_placeholder(self, prepend_shape, name=None):
        return tf.placeholder(dtype=self.sample_dtype(), shape=prepend_shape+self.sample_shape(), name=name)

    def __eq__(self, other):
        return (type(self) == type(other)) and (self.__dict__ == other.__dict__)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType">BernoulliPdType</a></li>
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType">CategoricalPdType</a></li>
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType">DiagGaussianPdType</a></li>
<li><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType">MultiCategoricalPdType</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType.param_placeholder"><code class="name flex">
<span>def <span class="ident">param_placeholder</span></span>(<span>self, prepend_shape, name=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def param_placeholder(self, prepend_shape, name=None):
    return tf.placeholder(dtype=tf.float32, shape=prepend_shape+self.param_shape(), name=name)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType.param_shape"><code class="name flex">
<span>def <span class="ident">param_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def param_shape(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdclass"><code class="name flex">
<span>def <span class="ident">pdclass</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdclass(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdfromflat"><code class="name flex">
<span>def <span class="ident">pdfromflat</span></span>(<span>self, flat)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdfromflat(self, flat):
    return self.pdclass()(flat)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdfromlatent"><code class="name flex">
<span>def <span class="ident">pdfromlatent</span></span>(<span>self, latent_vector, init_scale, init_bias)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdfromlatent(self, latent_vector, init_scale, init_bias):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_dtype"><code class="name flex">
<span>def <span class="ident">sample_dtype</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_dtype(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_placeholder"><code class="name flex">
<span>def <span class="ident">sample_placeholder</span></span>(<span>self, prepend_shape, name=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_placeholder(self, prepend_shape, name=None):
    return tf.placeholder(dtype=self.sample_dtype(), shape=prepend_shape+self.sample_shape(), name=name)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_shape"><code class="name flex">
<span>def <span class="ident">sample_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_shape(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">
<img src="https://github.com/flowersteam/TeachMyAgent/blob/gh-pages/images/home/head_image.png?raw=true" style="display: block; margin: 1em auto">
</a>
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">Home</a> | <a href="http://developmentalsystems.org/TeachMyAgent/">Website</a>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TeachMyAgent.students.openai_baselines.common" href="index.html">TeachMyAgent.students.openai_baselines.common</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.make_pdtype" href="#TeachMyAgent.students.openai_baselines.common.distributions.make_pdtype">make_pdtype</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.shape_el" href="#TeachMyAgent.students.openai_baselines.common.distributions.shape_el">shape_el</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.test_probtypes" href="#TeachMyAgent.students.openai_baselines.common.distributions.test_probtypes">test_probtypes</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.validate_probtype" href="#TeachMyAgent.students.openai_baselines.common.distributions.validate_probtype">validate_probtype</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd">BernoulliPd</a></code></h4>
<ul class="two-column">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.entropy" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.entropy">entropy</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.flatparam" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.flatparam">flatparam</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.fromflat" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.fromflat">fromflat</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.kl" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.kl">kl</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.mean" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.mean">mean</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.mode" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.mode">mode</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.neglogp" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.neglogp">neglogp</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.sample" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPd.sample">sample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType">BernoulliPdType</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.param_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.param_shape">param_shape</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.pdclass" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.pdclass">pdclass</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.pdfromlatent" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.pdfromlatent">pdfromlatent</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.sample_dtype" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.sample_dtype">sample_dtype</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.sample_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.BernoulliPdType.sample_shape">sample_shape</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd">CategoricalPd</a></code></h4>
<ul class="two-column">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.entropy" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.entropy">entropy</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.flatparam" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.flatparam">flatparam</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.fromflat" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.fromflat">fromflat</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.kl" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.kl">kl</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.mean" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.mean">mean</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.mode" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.mode">mode</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.neglogp" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.neglogp">neglogp</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.sample" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPd.sample">sample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType">CategoricalPdType</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.param_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.param_shape">param_shape</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.pdclass" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.pdclass">pdclass</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.pdfromlatent" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.pdfromlatent">pdfromlatent</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.sample_dtype" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.sample_dtype">sample_dtype</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.sample_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.CategoricalPdType.sample_shape">sample_shape</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd">DiagGaussianPd</a></code></h4>
<ul class="two-column">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.entropy" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.entropy">entropy</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.flatparam" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.flatparam">flatparam</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.fromflat" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.fromflat">fromflat</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.kl" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.kl">kl</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.mode" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.mode">mode</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.neglogp" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.neglogp">neglogp</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.sample" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPd.sample">sample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType">DiagGaussianPdType</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.param_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.param_shape">param_shape</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.pdclass" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.pdclass">pdclass</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.pdfromlatent" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.pdfromlatent">pdfromlatent</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.sample_dtype" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.sample_dtype">sample_dtype</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.sample_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.DiagGaussianPdType.sample_shape">sample_shape</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd">MultiCategoricalPd</a></code></h4>
<ul class="two-column">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.entropy" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.entropy">entropy</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.flatparam" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.flatparam">flatparam</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.fromflat" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.fromflat">fromflat</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.kl" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.kl">kl</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.mode" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.mode">mode</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.neglogp" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.neglogp">neglogp</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.sample" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPd.sample">sample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType">MultiCategoricalPdType</a></code></h4>
<ul class="two-column">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.param_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.param_shape">param_shape</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdclass" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdclass">pdclass</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdfromflat" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdfromflat">pdfromflat</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdfromlatent" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.pdfromlatent">pdfromlatent</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.sample_dtype" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.sample_dtype">sample_dtype</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.sample_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.MultiCategoricalPdType.sample_shape">sample_shape</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd">Pd</a></code></h4>
<ul class="two-column">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.entropy" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.entropy">entropy</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.flatparam" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.flatparam">flatparam</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.get_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.get_shape">get_shape</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.kl" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.kl">kl</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.logp" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.logp">logp</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.mode" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.mode">mode</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.neglogp" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.neglogp">neglogp</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.sample" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.sample">sample</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.Pd.shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.Pd.shape">shape</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType">PdType</a></code></h4>
<ul class="two-column">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType.param_placeholder" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType.param_placeholder">param_placeholder</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType.param_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType.param_shape">param_shape</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdclass" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdclass">pdclass</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdfromflat" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdfromflat">pdfromflat</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdfromlatent" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType.pdfromlatent">pdfromlatent</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_dtype" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_dtype">sample_dtype</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_placeholder" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_placeholder">sample_placeholder</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_shape" href="#TeachMyAgent.students.openai_baselines.common.distributions.PdType.sample_shape">sample_shape</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>