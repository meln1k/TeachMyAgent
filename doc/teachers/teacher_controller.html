<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>TeachMyAgent.teachers.teacher_controller API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="icon" href="https://github.com/flowersteam/TeachMyAgent/blob/gh-pages/images/favicon-96x96.png?raw=true" />
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TeachMyAgent.teachers.teacher_controller</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import pickle
import copy
from TeachMyAgent.teachers.algos.riac import RIAC
from TeachMyAgent.teachers.algos.alp_gmm import ALPGMM
from TeachMyAgent.teachers.algos.covar_gmm import CovarGMM
from TeachMyAgent.teachers.algos.adr import ADR
from TeachMyAgent.teachers.algos.self_paced_teacher import SelfPacedTeacher
from TeachMyAgent.teachers.algos.goal_gan import GoalGAN
from TeachMyAgent.teachers.algos.setter_solver import SetterSolver
from TeachMyAgent.teachers.algos.random_teacher import RandomTeacher
from TeachMyAgent.teachers.utils.dimensions_shuffler import DimensionsShuffler
from collections import OrderedDict

# Utils functions to convert task vector into dictionary (or the opposite)
def param_vec_to_param_dict(param_env_bounds, param):
    &#39;&#39;&#39;
        Convert a task vector into a dictionary.

        Args:
            param_env_bounds: Dictionary containing bounds of each dimension
            param: Task vector
        Returns:
            Task as a dictionary
    &#39;&#39;&#39;
    param_dict = OrderedDict()
    cpt = 0
    for i,(name, bounds) in enumerate(param_env_bounds.items()):
        if type(bounds[0]) is list:
            nb_dims = len(bounds)
            param_dict[name] = param[cpt:cpt+nb_dims]
            cpt += nb_dims
        else:
            if len(bounds) == 2:
                param_dict[name] = param[cpt]
                cpt += 1
            elif len(bounds) == 3:  # third value is the number of dimensions having these bounds
                nb_dims = bounds[2]
                param_dict[name] = param[cpt:cpt+nb_dims]
                cpt += nb_dims

    return param_dict

def param_dict_to_param_vec(param_env_bounds, param_dict):
    &#39;&#39;&#39;
        Convert a task dictionary into a vector.

        Args:
            param_env_bounds: Dictionary containing bounds of each dimension
            param_dict: Task dictionary
        Returns:
            Task vector
    &#39;&#39;&#39;
    param_vec = []
    for name, bounds in param_env_bounds.items():
        if isinstance(param_dict[name], list) or isinstance(param_dict[name], np.ndarray):
            param_vec.extend(param_dict[name])
        else:
            param_vec.append(param_dict[name])

    return np.array(param_vec)

# Class controlling the interactions between ACL methods and DeepRL students
class TeacherController(object):
    &#39;&#39;&#39;
        Control the interactions between the ACL method and the DeepRL student.
    &#39;&#39;&#39;
    def __init__(self, teacher, nb_test_episodes, param_env_bounds, seed=None, test_set=None,
                 keep_periodical_task_samples=None, shuffle_dimensions=False, scale_reward=False, **teacher_params):
        &#39;&#39;&#39;
            Create a TeacherController to make and ACL method interact with a DeepRL student.
            Pass this object to your DeepRL student and use the methods below to as for new tasks and record trajectories.

            Args:
                teacher (str): Teacher&#39;s name
                nb_test_episodes: Number of episodes in the test set (used if a new test set must be generated)
                param_env_bounds (dict): Bounds of the task space
                seed: Seed for the teacher and the test set generation (if needed)
                test_set: Test set&#39;s name if an existing test set must be used.
                          Saved test set are in the `TeachMyAgent/teachers/test_sets` folder.
                          Just specify the filename without the path (and without the .pkl extension).
                keep_periodical_task_samples: How frequently the teacher must be asked to sample 100 (non exploratory) tasks.
                                              This is then used to visualize the curriculum.
                shuffle_dimensions (bool): Whether the task space the ACL method uses should be cut into hypercubes and shuffled.
                                    If set to True, the ACL teacher interacts with  the shuffled task space.
                                    Tasks are then mapped towards the real task space using a DimensionsShuffler.
                scale_reward (bool): Whether rewards should be scaled to a [0, 1] interval
                teacher_params: Additional kwargs for the ACL method.
        &#39;&#39;&#39;
        self.teacher = teacher
        self.nb_test_episodes = nb_test_episodes
        self.test_set = test_set
        self.test_ep_counter = 0
        self.train_step_counter = 0
        self.eps= 1e-03
        self.param_env_bounds = copy.deepcopy(param_env_bounds)
        self.keep_periodical_task_samples = keep_periodical_task_samples
        self.scale_reward = scale_reward

        # figure out parameters boundaries vectors
        mins, maxs = [], []
        for name, bounds in param_env_bounds.items():
            if type(bounds[0]) is list:
                try:
                    # Define min / max for each dim
                    for dim in bounds:
                        mins.append(dim[0])
                        maxs.append(dim[1])
                except:
                    print(&#34;ill defined boundaries, use [min, max, nb_dims] format or [min, max] if nb_dims=1&#34;)
                    exit(1)
            else:
                if len(bounds) == 2:
                    mins.append(bounds[0])
                    maxs.append(bounds[1])
                elif len(bounds) == 3:  # third value is the number of dimensions having these bounds
                    mins.extend([bounds[0]] * bounds[2])
                    maxs.extend([bounds[1]] * bounds[2])
                else:
                    print(&#34;ill defined boundaries, use [min, max, nb_dims] format or [min, max] if nb_dims=1&#34;)
                    exit(1)
        self.task_dim = len(mins)

        # If `shuffle_dimensions` is set to True, the ACL teacher interacts with tasks in the shuffled task space.
        # Tasks are then mapped towards the real task space.
        # This uses a DimensionsShuffler.
        if shuffle_dimensions:
            self.dimensions_shuffler = DimensionsShuffler(mins, maxs, seed=seed)
            if &#34;initial_dist&#34; in teacher_params:
                teacher_params[&#34;initial_dist&#34;][&#34;mean&#34;] = self.dimensions_shuffler.inverse_interpolate_task(
                    teacher_params[&#34;initial_dist&#34;][&#34;mean&#34;])
            if &#34;target_dist&#34; in teacher_params:
                    teacher_params[&#34;target_dist&#34;][&#34;mean&#34;] = self.dimensions_shuffler.inverse_interpolate_task(
                        teacher_params[&#34;target_dist&#34;][&#34;mean&#34;])
        else:
            self.dimensions_shuffler = None

        # setup tasks generator
        if teacher == &#39;Random&#39;:
            self.task_generator = RandomTeacher(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;RIAC&#39;:
            self.task_generator = RIAC(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;ALP-GMM&#39;:
            self.task_generator = ALPGMM(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;Covar-GMM&#39;:
            self.task_generator = CovarGMM(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;ADR&#39;:
            self.task_generator = ADR(mins, maxs, seed=seed, scale_reward=scale_reward, **teacher_params)
        elif teacher == &#39;Self-Paced&#39;:
            self.task_generator = SelfPacedTeacher(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;GoalGAN&#39;:
            self.task_generator = GoalGAN(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;Setter-Solver&#39;:
            self.task_generator = SetterSolver(mins, maxs, seed=seed, **teacher_params)
        else:
            print(&#39;Unknown teacher&#39;)
            raise NotImplementedError

        # Generate test set
        ## Use evenly distributed tasks for StumpTracks
        ## Use uniform sampling otherwise
        ## Or load a saved test set
        test_param_vec = None
        if test_set is None:
            if self.task_dim == 2 and &#34;stump_height&#34; in param_env_bounds and &#34;obstacle_spacing&#34; in param_env_bounds: # StumpTracks
                print(&#34;Using random test set for two fixed dimensions.&#34;)
                # select &lt;nb_test_episodes&gt; parameters choosen uniformly in the task space
                nb_steps = int(nb_test_episodes ** (1 / self.task_dim))
                d1 = np.linspace(mins[0], maxs[0], nb_steps, endpoint=True)
                d2 = np.linspace(mins[1], maxs[1], nb_steps, endpoint=True)
                test_param_vec = np.transpose([np.tile(d1, len(d2)), np.repeat(d2, len(d1))])  # cartesian product
            else:
                print(&#34;Using random test set.&#34;)
                test_random_state = np.random.RandomState(
                    31)  # Seed a new random generator not impacting the global one to always get the same test set
                test_param_vec = test_random_state.uniform(mins, maxs, size=(nb_test_episodes, self.task_dim))
        else:
            test_param_vec = np.array(pickle.load(open(&#34;TeachMyAgent/teachers/test_sets/&#34;+test_set+&#34;.pkl&#34;, &#34;rb&#34;)))
            self.nb_test_episodes = len(test_param_vec)
            print(&#39;fixed set of {} tasks loaded&#39;.format(len(test_param_vec)))
        test_param_dicts = [param_vec_to_param_dict(param_env_bounds, vec) for vec in test_param_vec]
        self.test_env_list = test_param_dicts

        # Data recording
        self.env_params_train = []
        self.env_train_rewards = []
        self.env_train_norm_rewards = []
        self.env_train_len = []
        self.periodical_task_samples = []
        self.periodical_task_infos = []

        self.env_params_test = []
        self.env_test_rewards = []
        self.env_test_len = []

    def _get_last_task(self):
        params = self.env_params_train[-1]
        if self.dimensions_shuffler is not None:
            params = self.dimensions_shuffler.last_raw_task
        return params

    def set_value_estimator(self, estimator):
        &#39;&#39;&#39;
            Give the DeepRL value estimator to the teacher.
        &#39;&#39;&#39;
        self.task_generator.value_estimator = estimator

    def record_train_task_initial_state(self, initial_state):
        &#39;&#39;&#39;
            Record the initial state of the lastly sampled task.
        &#39;&#39;&#39;
        self.task_generator.record_initial_state(self._get_last_task(), initial_state)

    def record_train_step(self, state, action, reward, next_state, done):
        &#39;&#39;&#39;
            Record a step for the last task.
        &#39;&#39;&#39;
        self.train_step_counter += 1
        self.task_generator.step_update(state, action, reward, next_state, done)
        # Monitor curriculum
        if self.keep_periodical_task_samples is not None \
                and self.train_step_counter % self.keep_periodical_task_samples == 0:
            tasks = []
            infos = []
            if self.task_generator.is_non_exploratory_task_sampling_available():
                for i in range(100):
                    task_and_infos = self.task_generator.non_exploratory_task_sampling()
                    tasks.append(task_and_infos[&#34;task&#34;])
                    infos.append(task_and_infos[&#34;infos&#34;])
            self.periodical_task_samples.append(np.array(tasks))
            self.periodical_task_infos.append(np.array(infos))

    def record_train_episode(self, ep_reward, ep_len, is_success=False):
        &#39;&#39;&#39;
            Record the episode associated to the last task.

                ep_reward: Return
                ep_len: Number of steps done
                is_success: Binary reward
        &#39;&#39;&#39;
        self.env_train_rewards.append(ep_reward)
        self.env_train_len.append(ep_len)
        if self.scale_reward:
            ep_reward = np.interp(ep_reward,
                                  (self.task_generator.env_reward_lb, self.task_generator.env_reward_ub),
                                  (0, 1))
            self.env_train_norm_rewards.append(ep_reward)
        self.task_generator.episodic_update(self._get_last_task(), ep_reward, is_success)

    def record_test_episode(self, reward, ep_len):
        &#39;&#39;&#39;
            Record the episode for the last test task sampled.
        &#39;&#39;&#39;
        self.env_test_rewards.append(reward)
        self.env_test_len.append(ep_len)

    def dump(self, filename):
        &#39;&#39;&#39;
            Save teacher and all book-keeped information.
        &#39;&#39;&#39;
        with open(filename, &#39;wb&#39;) as handle:
            dump_dict = {&#39;env_params_train&#39;: self.env_params_train,
                         &#39;env_train_rewards&#39;: self.env_train_rewards,
                         &#39;env_train_len&#39;: self.env_train_len,
                         &#39;env_params_test&#39;: self.env_params_test,
                         &#39;env_test_rewards&#39;: self.env_test_rewards,
                         &#39;env_test_len&#39;: self.env_test_len,
                         &#39;env_param_bounds&#39;: list(self.param_env_bounds.items()),
                         &#39;periodical_samples&#39;: self.periodical_task_samples,
                         &#39;periodical_infos&#39;: self.periodical_task_infos}
            dump_dict = self.task_generator.dump(dump_dict)
            pickle.dump(dump_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def set_env_params(self, env):
        &#39;&#39;&#39;
            Sample a task and set the PCG of the environment with it.
        &#39;&#39;&#39;
        params = copy.copy(self.task_generator.sample_task())
        if self.dimensions_shuffler is not None:
            params = self.dimensions_shuffler.interpolate_task(params)
        self.env_params_train.append(params)
        if len(params) &gt; 0:
            assert type(params[0]) == np.float32
            param_dict = param_vec_to_param_dict(self.param_env_bounds, params)
            env.set_environment(**param_dict)
        return params

    def set_test_env_params(self, test_env):
        &#39;&#39;&#39;
            Sample a test task from the test set and set the PCG of the test environment with it.
        &#39;&#39;&#39;
        self.test_ep_counter += 1
        test_param_dict = self.test_env_list[self.test_ep_counter - 1]

        if self.test_set == &#34;hexagon_test_set&#34;:
            # removing legacy parameters from test_set, don&#39;t pay attention
            legacy = [&#39;tunnel_height&#39;, &#39;gap_width&#39;, &#39;step_height&#39;, &#39;step_number&#39;]
            keys = test_param_dict.keys()
            for env_param in legacy:
                if env_param in keys:
                    del test_param_dict[env_param]

        test_param_vec = param_dict_to_param_vec(self.param_env_bounds, test_param_dict)
        if len(test_param_vec) &gt; 0:
            self.env_params_test.append(test_param_vec)
            test_env.set_environment(**test_param_dict)

        if self.test_ep_counter == self.nb_test_episodes:
            self.test_ep_counter = 0
        return test_param_dict</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="TeachMyAgent.teachers.teacher_controller.param_dict_to_param_vec"><code class="name flex">
<span>def <span class="ident">param_dict_to_param_vec</span></span>(<span>param_env_bounds, param_dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a task dictionary into a vector.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>param_env_bounds</code></strong></dt>
<dd>Dictionary containing bounds of each dimension</dd>
<dt><strong><code>param_dict</code></strong></dt>
<dd>Task dictionary</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Task vector</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def param_dict_to_param_vec(param_env_bounds, param_dict):
    &#39;&#39;&#39;
        Convert a task dictionary into a vector.

        Args:
            param_env_bounds: Dictionary containing bounds of each dimension
            param_dict: Task dictionary
        Returns:
            Task vector
    &#39;&#39;&#39;
    param_vec = []
    for name, bounds in param_env_bounds.items():
        if isinstance(param_dict[name], list) or isinstance(param_dict[name], np.ndarray):
            param_vec.extend(param_dict[name])
        else:
            param_vec.append(param_dict[name])

    return np.array(param_vec)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.teachers.teacher_controller.param_vec_to_param_dict"><code class="name flex">
<span>def <span class="ident">param_vec_to_param_dict</span></span>(<span>param_env_bounds, param)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a task vector into a dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>param_env_bounds</code></strong></dt>
<dd>Dictionary containing bounds of each dimension</dd>
<dt><strong><code>param</code></strong></dt>
<dd>Task vector</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Task as a dictionary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def param_vec_to_param_dict(param_env_bounds, param):
    &#39;&#39;&#39;
        Convert a task vector into a dictionary.

        Args:
            param_env_bounds: Dictionary containing bounds of each dimension
            param: Task vector
        Returns:
            Task as a dictionary
    &#39;&#39;&#39;
    param_dict = OrderedDict()
    cpt = 0
    for i,(name, bounds) in enumerate(param_env_bounds.items()):
        if type(bounds[0]) is list:
            nb_dims = len(bounds)
            param_dict[name] = param[cpt:cpt+nb_dims]
            cpt += nb_dims
        else:
            if len(bounds) == 2:
                param_dict[name] = param[cpt]
                cpt += 1
            elif len(bounds) == 3:  # third value is the number of dimensions having these bounds
                nb_dims = bounds[2]
                param_dict[name] = param[cpt:cpt+nb_dims]
                cpt += nb_dims

    return param_dict</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController"><code class="flex name class">
<span>class <span class="ident">TeacherController</span></span>
<span>(</span><span>teacher, nb_test_episodes, param_env_bounds, seed=None, test_set=None, keep_periodical_task_samples=None, shuffle_dimensions=False, scale_reward=False, **teacher_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Control the interactions between the ACL method and the DeepRL student.</p>
<p>Create a TeacherController to make and ACL method interact with a DeepRL student.
Pass this object to your DeepRL student and use the methods below to as for new tasks and record trajectories.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>teacher</code></strong> :&ensp;<code>str</code></dt>
<dd>Teacher's name</dd>
<dt><strong><code>nb_test_episodes</code></strong></dt>
<dd>Number of episodes in the test set (used if a new test set must be generated)</dd>
<dt><strong><code>param_env_bounds</code></strong> :&ensp;<code>dict</code></dt>
<dd>Bounds of the task space</dd>
<dt><strong><code>seed</code></strong></dt>
<dd>Seed for the teacher and the test set generation (if needed)</dd>
<dt><strong><code>test_set</code></strong></dt>
<dd>Test set's name if an existing test set must be used.
Saved test set are in the <code>TeachMyAgent/teachers/test_sets</code> folder.
Just specify the filename without the path (and without the .pkl extension).</dd>
<dt><strong><code>keep_periodical_task_samples</code></strong></dt>
<dd>How frequently the teacher must be asked to sample 100 (non exploratory) tasks.
This is then used to visualize the curriculum.</dd>
<dt><strong><code>shuffle_dimensions</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the task space the ACL method uses should be cut into hypercubes and shuffled.
If set to True, the ACL teacher interacts with
the shuffled task space.
Tasks are then mapped towards the real task space using a DimensionsShuffler.</dd>
<dt><strong><code>scale_reward</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether rewards should be scaled to a [0, 1] interval</dd>
<dt><strong><code>teacher_params</code></strong></dt>
<dd>Additional kwargs for the ACL method.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TeacherController(object):
    &#39;&#39;&#39;
        Control the interactions between the ACL method and the DeepRL student.
    &#39;&#39;&#39;
    def __init__(self, teacher, nb_test_episodes, param_env_bounds, seed=None, test_set=None,
                 keep_periodical_task_samples=None, shuffle_dimensions=False, scale_reward=False, **teacher_params):
        &#39;&#39;&#39;
            Create a TeacherController to make and ACL method interact with a DeepRL student.
            Pass this object to your DeepRL student and use the methods below to as for new tasks and record trajectories.

            Args:
                teacher (str): Teacher&#39;s name
                nb_test_episodes: Number of episodes in the test set (used if a new test set must be generated)
                param_env_bounds (dict): Bounds of the task space
                seed: Seed for the teacher and the test set generation (if needed)
                test_set: Test set&#39;s name if an existing test set must be used.
                          Saved test set are in the `TeachMyAgent/teachers/test_sets` folder.
                          Just specify the filename without the path (and without the .pkl extension).
                keep_periodical_task_samples: How frequently the teacher must be asked to sample 100 (non exploratory) tasks.
                                              This is then used to visualize the curriculum.
                shuffle_dimensions (bool): Whether the task space the ACL method uses should be cut into hypercubes and shuffled.
                                    If set to True, the ACL teacher interacts with  the shuffled task space.
                                    Tasks are then mapped towards the real task space using a DimensionsShuffler.
                scale_reward (bool): Whether rewards should be scaled to a [0, 1] interval
                teacher_params: Additional kwargs for the ACL method.
        &#39;&#39;&#39;
        self.teacher = teacher
        self.nb_test_episodes = nb_test_episodes
        self.test_set = test_set
        self.test_ep_counter = 0
        self.train_step_counter = 0
        self.eps= 1e-03
        self.param_env_bounds = copy.deepcopy(param_env_bounds)
        self.keep_periodical_task_samples = keep_periodical_task_samples
        self.scale_reward = scale_reward

        # figure out parameters boundaries vectors
        mins, maxs = [], []
        for name, bounds in param_env_bounds.items():
            if type(bounds[0]) is list:
                try:
                    # Define min / max for each dim
                    for dim in bounds:
                        mins.append(dim[0])
                        maxs.append(dim[1])
                except:
                    print(&#34;ill defined boundaries, use [min, max, nb_dims] format or [min, max] if nb_dims=1&#34;)
                    exit(1)
            else:
                if len(bounds) == 2:
                    mins.append(bounds[0])
                    maxs.append(bounds[1])
                elif len(bounds) == 3:  # third value is the number of dimensions having these bounds
                    mins.extend([bounds[0]] * bounds[2])
                    maxs.extend([bounds[1]] * bounds[2])
                else:
                    print(&#34;ill defined boundaries, use [min, max, nb_dims] format or [min, max] if nb_dims=1&#34;)
                    exit(1)
        self.task_dim = len(mins)

        # If `shuffle_dimensions` is set to True, the ACL teacher interacts with tasks in the shuffled task space.
        # Tasks are then mapped towards the real task space.
        # This uses a DimensionsShuffler.
        if shuffle_dimensions:
            self.dimensions_shuffler = DimensionsShuffler(mins, maxs, seed=seed)
            if &#34;initial_dist&#34; in teacher_params:
                teacher_params[&#34;initial_dist&#34;][&#34;mean&#34;] = self.dimensions_shuffler.inverse_interpolate_task(
                    teacher_params[&#34;initial_dist&#34;][&#34;mean&#34;])
            if &#34;target_dist&#34; in teacher_params:
                    teacher_params[&#34;target_dist&#34;][&#34;mean&#34;] = self.dimensions_shuffler.inverse_interpolate_task(
                        teacher_params[&#34;target_dist&#34;][&#34;mean&#34;])
        else:
            self.dimensions_shuffler = None

        # setup tasks generator
        if teacher == &#39;Random&#39;:
            self.task_generator = RandomTeacher(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;RIAC&#39;:
            self.task_generator = RIAC(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;ALP-GMM&#39;:
            self.task_generator = ALPGMM(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;Covar-GMM&#39;:
            self.task_generator = CovarGMM(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;ADR&#39;:
            self.task_generator = ADR(mins, maxs, seed=seed, scale_reward=scale_reward, **teacher_params)
        elif teacher == &#39;Self-Paced&#39;:
            self.task_generator = SelfPacedTeacher(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;GoalGAN&#39;:
            self.task_generator = GoalGAN(mins, maxs, seed=seed, **teacher_params)
        elif teacher == &#39;Setter-Solver&#39;:
            self.task_generator = SetterSolver(mins, maxs, seed=seed, **teacher_params)
        else:
            print(&#39;Unknown teacher&#39;)
            raise NotImplementedError

        # Generate test set
        ## Use evenly distributed tasks for StumpTracks
        ## Use uniform sampling otherwise
        ## Or load a saved test set
        test_param_vec = None
        if test_set is None:
            if self.task_dim == 2 and &#34;stump_height&#34; in param_env_bounds and &#34;obstacle_spacing&#34; in param_env_bounds: # StumpTracks
                print(&#34;Using random test set for two fixed dimensions.&#34;)
                # select &lt;nb_test_episodes&gt; parameters choosen uniformly in the task space
                nb_steps = int(nb_test_episodes ** (1 / self.task_dim))
                d1 = np.linspace(mins[0], maxs[0], nb_steps, endpoint=True)
                d2 = np.linspace(mins[1], maxs[1], nb_steps, endpoint=True)
                test_param_vec = np.transpose([np.tile(d1, len(d2)), np.repeat(d2, len(d1))])  # cartesian product
            else:
                print(&#34;Using random test set.&#34;)
                test_random_state = np.random.RandomState(
                    31)  # Seed a new random generator not impacting the global one to always get the same test set
                test_param_vec = test_random_state.uniform(mins, maxs, size=(nb_test_episodes, self.task_dim))
        else:
            test_param_vec = np.array(pickle.load(open(&#34;TeachMyAgent/teachers/test_sets/&#34;+test_set+&#34;.pkl&#34;, &#34;rb&#34;)))
            self.nb_test_episodes = len(test_param_vec)
            print(&#39;fixed set of {} tasks loaded&#39;.format(len(test_param_vec)))
        test_param_dicts = [param_vec_to_param_dict(param_env_bounds, vec) for vec in test_param_vec]
        self.test_env_list = test_param_dicts

        # Data recording
        self.env_params_train = []
        self.env_train_rewards = []
        self.env_train_norm_rewards = []
        self.env_train_len = []
        self.periodical_task_samples = []
        self.periodical_task_infos = []

        self.env_params_test = []
        self.env_test_rewards = []
        self.env_test_len = []

    def _get_last_task(self):
        params = self.env_params_train[-1]
        if self.dimensions_shuffler is not None:
            params = self.dimensions_shuffler.last_raw_task
        return params

    def set_value_estimator(self, estimator):
        &#39;&#39;&#39;
            Give the DeepRL value estimator to the teacher.
        &#39;&#39;&#39;
        self.task_generator.value_estimator = estimator

    def record_train_task_initial_state(self, initial_state):
        &#39;&#39;&#39;
            Record the initial state of the lastly sampled task.
        &#39;&#39;&#39;
        self.task_generator.record_initial_state(self._get_last_task(), initial_state)

    def record_train_step(self, state, action, reward, next_state, done):
        &#39;&#39;&#39;
            Record a step for the last task.
        &#39;&#39;&#39;
        self.train_step_counter += 1
        self.task_generator.step_update(state, action, reward, next_state, done)
        # Monitor curriculum
        if self.keep_periodical_task_samples is not None \
                and self.train_step_counter % self.keep_periodical_task_samples == 0:
            tasks = []
            infos = []
            if self.task_generator.is_non_exploratory_task_sampling_available():
                for i in range(100):
                    task_and_infos = self.task_generator.non_exploratory_task_sampling()
                    tasks.append(task_and_infos[&#34;task&#34;])
                    infos.append(task_and_infos[&#34;infos&#34;])
            self.periodical_task_samples.append(np.array(tasks))
            self.periodical_task_infos.append(np.array(infos))

    def record_train_episode(self, ep_reward, ep_len, is_success=False):
        &#39;&#39;&#39;
            Record the episode associated to the last task.

                ep_reward: Return
                ep_len: Number of steps done
                is_success: Binary reward
        &#39;&#39;&#39;
        self.env_train_rewards.append(ep_reward)
        self.env_train_len.append(ep_len)
        if self.scale_reward:
            ep_reward = np.interp(ep_reward,
                                  (self.task_generator.env_reward_lb, self.task_generator.env_reward_ub),
                                  (0, 1))
            self.env_train_norm_rewards.append(ep_reward)
        self.task_generator.episodic_update(self._get_last_task(), ep_reward, is_success)

    def record_test_episode(self, reward, ep_len):
        &#39;&#39;&#39;
            Record the episode for the last test task sampled.
        &#39;&#39;&#39;
        self.env_test_rewards.append(reward)
        self.env_test_len.append(ep_len)

    def dump(self, filename):
        &#39;&#39;&#39;
            Save teacher and all book-keeped information.
        &#39;&#39;&#39;
        with open(filename, &#39;wb&#39;) as handle:
            dump_dict = {&#39;env_params_train&#39;: self.env_params_train,
                         &#39;env_train_rewards&#39;: self.env_train_rewards,
                         &#39;env_train_len&#39;: self.env_train_len,
                         &#39;env_params_test&#39;: self.env_params_test,
                         &#39;env_test_rewards&#39;: self.env_test_rewards,
                         &#39;env_test_len&#39;: self.env_test_len,
                         &#39;env_param_bounds&#39;: list(self.param_env_bounds.items()),
                         &#39;periodical_samples&#39;: self.periodical_task_samples,
                         &#39;periodical_infos&#39;: self.periodical_task_infos}
            dump_dict = self.task_generator.dump(dump_dict)
            pickle.dump(dump_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def set_env_params(self, env):
        &#39;&#39;&#39;
            Sample a task and set the PCG of the environment with it.
        &#39;&#39;&#39;
        params = copy.copy(self.task_generator.sample_task())
        if self.dimensions_shuffler is not None:
            params = self.dimensions_shuffler.interpolate_task(params)
        self.env_params_train.append(params)
        if len(params) &gt; 0:
            assert type(params[0]) == np.float32
            param_dict = param_vec_to_param_dict(self.param_env_bounds, params)
            env.set_environment(**param_dict)
        return params

    def set_test_env_params(self, test_env):
        &#39;&#39;&#39;
            Sample a test task from the test set and set the PCG of the test environment with it.
        &#39;&#39;&#39;
        self.test_ep_counter += 1
        test_param_dict = self.test_env_list[self.test_ep_counter - 1]

        if self.test_set == &#34;hexagon_test_set&#34;:
            # removing legacy parameters from test_set, don&#39;t pay attention
            legacy = [&#39;tunnel_height&#39;, &#39;gap_width&#39;, &#39;step_height&#39;, &#39;step_number&#39;]
            keys = test_param_dict.keys()
            for env_param in legacy:
                if env_param in keys:
                    del test_param_dict[env_param]

        test_param_vec = param_dict_to_param_vec(self.param_env_bounds, test_param_dict)
        if len(test_param_vec) &gt; 0:
            self.env_params_test.append(test_param_vec)
            test_env.set_environment(**test_param_dict)

        if self.test_ep_counter == self.nb_test_episodes:
            self.test_ep_counter = 0
        return test_param_dict</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController.dump"><code class="name flex">
<span>def <span class="ident">dump</span></span>(<span>self, filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Save teacher and all book-keeped information.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump(self, filename):
    &#39;&#39;&#39;
        Save teacher and all book-keeped information.
    &#39;&#39;&#39;
    with open(filename, &#39;wb&#39;) as handle:
        dump_dict = {&#39;env_params_train&#39;: self.env_params_train,
                     &#39;env_train_rewards&#39;: self.env_train_rewards,
                     &#39;env_train_len&#39;: self.env_train_len,
                     &#39;env_params_test&#39;: self.env_params_test,
                     &#39;env_test_rewards&#39;: self.env_test_rewards,
                     &#39;env_test_len&#39;: self.env_test_len,
                     &#39;env_param_bounds&#39;: list(self.param_env_bounds.items()),
                     &#39;periodical_samples&#39;: self.periodical_task_samples,
                     &#39;periodical_infos&#39;: self.periodical_task_infos}
        dump_dict = self.task_generator.dump(dump_dict)
        pickle.dump(dump_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController.record_test_episode"><code class="name flex">
<span>def <span class="ident">record_test_episode</span></span>(<span>self, reward, ep_len)</span>
</code></dt>
<dd>
<div class="desc"><p>Record the episode for the last test task sampled.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def record_test_episode(self, reward, ep_len):
    &#39;&#39;&#39;
        Record the episode for the last test task sampled.
    &#39;&#39;&#39;
    self.env_test_rewards.append(reward)
    self.env_test_len.append(ep_len)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_episode"><code class="name flex">
<span>def <span class="ident">record_train_episode</span></span>(<span>self, ep_reward, ep_len, is_success=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Record the episode associated to the last task.</p>
<pre><code>ep_reward: Return
ep_len: Number of steps done
is_success: Binary reward
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def record_train_episode(self, ep_reward, ep_len, is_success=False):
    &#39;&#39;&#39;
        Record the episode associated to the last task.

            ep_reward: Return
            ep_len: Number of steps done
            is_success: Binary reward
    &#39;&#39;&#39;
    self.env_train_rewards.append(ep_reward)
    self.env_train_len.append(ep_len)
    if self.scale_reward:
        ep_reward = np.interp(ep_reward,
                              (self.task_generator.env_reward_lb, self.task_generator.env_reward_ub),
                              (0, 1))
        self.env_train_norm_rewards.append(ep_reward)
    self.task_generator.episodic_update(self._get_last_task(), ep_reward, is_success)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_step"><code class="name flex">
<span>def <span class="ident">record_train_step</span></span>(<span>self, state, action, reward, next_state, done)</span>
</code></dt>
<dd>
<div class="desc"><p>Record a step for the last task.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def record_train_step(self, state, action, reward, next_state, done):
    &#39;&#39;&#39;
        Record a step for the last task.
    &#39;&#39;&#39;
    self.train_step_counter += 1
    self.task_generator.step_update(state, action, reward, next_state, done)
    # Monitor curriculum
    if self.keep_periodical_task_samples is not None \
            and self.train_step_counter % self.keep_periodical_task_samples == 0:
        tasks = []
        infos = []
        if self.task_generator.is_non_exploratory_task_sampling_available():
            for i in range(100):
                task_and_infos = self.task_generator.non_exploratory_task_sampling()
                tasks.append(task_and_infos[&#34;task&#34;])
                infos.append(task_and_infos[&#34;infos&#34;])
        self.periodical_task_samples.append(np.array(tasks))
        self.periodical_task_infos.append(np.array(infos))</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_task_initial_state"><code class="name flex">
<span>def <span class="ident">record_train_task_initial_state</span></span>(<span>self, initial_state)</span>
</code></dt>
<dd>
<div class="desc"><p>Record the initial state of the lastly sampled task.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def record_train_task_initial_state(self, initial_state):
    &#39;&#39;&#39;
        Record the initial state of the lastly sampled task.
    &#39;&#39;&#39;
    self.task_generator.record_initial_state(self._get_last_task(), initial_state)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController.set_env_params"><code class="name flex">
<span>def <span class="ident">set_env_params</span></span>(<span>self, env)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample a task and set the PCG of the environment with it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_env_params(self, env):
    &#39;&#39;&#39;
        Sample a task and set the PCG of the environment with it.
    &#39;&#39;&#39;
    params = copy.copy(self.task_generator.sample_task())
    if self.dimensions_shuffler is not None:
        params = self.dimensions_shuffler.interpolate_task(params)
    self.env_params_train.append(params)
    if len(params) &gt; 0:
        assert type(params[0]) == np.float32
        param_dict = param_vec_to_param_dict(self.param_env_bounds, params)
        env.set_environment(**param_dict)
    return params</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController.set_test_env_params"><code class="name flex">
<span>def <span class="ident">set_test_env_params</span></span>(<span>self, test_env)</span>
</code></dt>
<dd>
<div class="desc"><p>Sample a test task from the test set and set the PCG of the test environment with it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_test_env_params(self, test_env):
    &#39;&#39;&#39;
        Sample a test task from the test set and set the PCG of the test environment with it.
    &#39;&#39;&#39;
    self.test_ep_counter += 1
    test_param_dict = self.test_env_list[self.test_ep_counter - 1]

    if self.test_set == &#34;hexagon_test_set&#34;:
        # removing legacy parameters from test_set, don&#39;t pay attention
        legacy = [&#39;tunnel_height&#39;, &#39;gap_width&#39;, &#39;step_height&#39;, &#39;step_number&#39;]
        keys = test_param_dict.keys()
        for env_param in legacy:
            if env_param in keys:
                del test_param_dict[env_param]

    test_param_vec = param_dict_to_param_vec(self.param_env_bounds, test_param_dict)
    if len(test_param_vec) &gt; 0:
        self.env_params_test.append(test_param_vec)
        test_env.set_environment(**test_param_dict)

    if self.test_ep_counter == self.nb_test_episodes:
        self.test_ep_counter = 0
    return test_param_dict</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.teachers.teacher_controller.TeacherController.set_value_estimator"><code class="name flex">
<span>def <span class="ident">set_value_estimator</span></span>(<span>self, estimator)</span>
</code></dt>
<dd>
<div class="desc"><p>Give the DeepRL value estimator to the teacher.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_value_estimator(self, estimator):
    &#39;&#39;&#39;
        Give the DeepRL value estimator to the teacher.
    &#39;&#39;&#39;
    self.task_generator.value_estimator = estimator</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">
<img src="https://github.com/flowersteam/TeachMyAgent/blob/gh-pages/images/home/head_image.png?raw=true" style="display: block; margin: 1em auto">
</a>
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">Home</a> | <a href="http://developmentalsystems.org/TeachMyAgent/">Website</a>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TeachMyAgent.teachers" href="index.html">TeachMyAgent.teachers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="TeachMyAgent.teachers.teacher_controller.param_dict_to_param_vec" href="#TeachMyAgent.teachers.teacher_controller.param_dict_to_param_vec">param_dict_to_param_vec</a></code></li>
<li><code><a title="TeachMyAgent.teachers.teacher_controller.param_vec_to_param_dict" href="#TeachMyAgent.teachers.teacher_controller.param_vec_to_param_dict">param_vec_to_param_dict</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController" href="#TeachMyAgent.teachers.teacher_controller.TeacherController">TeacherController</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController.dump" href="#TeachMyAgent.teachers.teacher_controller.TeacherController.dump">dump</a></code></li>
<li><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController.record_test_episode" href="#TeachMyAgent.teachers.teacher_controller.TeacherController.record_test_episode">record_test_episode</a></code></li>
<li><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_episode" href="#TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_episode">record_train_episode</a></code></li>
<li><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_step" href="#TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_step">record_train_step</a></code></li>
<li><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_task_initial_state" href="#TeachMyAgent.teachers.teacher_controller.TeacherController.record_train_task_initial_state">record_train_task_initial_state</a></code></li>
<li><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController.set_env_params" href="#TeachMyAgent.teachers.teacher_controller.TeacherController.set_env_params">set_env_params</a></code></li>
<li><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController.set_test_env_params" href="#TeachMyAgent.teachers.teacher_controller.TeacherController.set_test_env_params">set_test_env_params</a></code></li>
<li><code><a title="TeachMyAgent.teachers.teacher_controller.TeacherController.set_value_estimator" href="#TeachMyAgent.teachers.teacher_controller.TeacherController.set_value_estimator">set_value_estimator</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>